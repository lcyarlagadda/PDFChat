{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Dependencies\n",
        "%pip install streamlit pymupdf sentence-transformers faiss-cpu transformers accelerate bitsandbytes pyngrok python-dotenv tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Authentication\n",
        "import os\n",
        "from getpass import getpass\n",
        "from dotenv import load_dotenv\n",
        "from pyngrok import ngrok\n",
        "import huggingface_hub\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Get Hugging Face token\n",
        "hf_token = getpass(\"Enter your Hugging Face token: \")\n",
        "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = hf_token\n",
        "\n",
        "# Login to Hugging Face\n",
        "huggingface_hub.login(token=hf_token)\n",
        "\n",
        "# Get ngrok token\n",
        "ngrok_token = os.getenv(\"NGROK_AUTH_TOKEN\")\n",
        "if ngrok_token:\n",
        "    !ngrok authtoken $ngrok_token\n",
        "    print(\"Authentication setup complete!\")\n",
        "else:\n",
        "    print(\"Please set NGROK_AUTH_TOKEN in .env file\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import fitz\n",
        "import re\n",
        "import os\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import faiss\n",
        "from typing import List, Dict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "st.set_page_config(page_title=\"RAG PDF Chat\", layout=\"wide\")\n",
        "\n",
        "# CSS Styling\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    header[data-testid=\"stHeader\"] { display: none; }\n",
        "    .stApp > div:first-child { padding-top: 0; }\n",
        "    .main-container { max-width: 800px; margin: 0 auto; padding: 20px; padding-bottom: 100px; }\n",
        "    .header { text-align: center; color: #ffffff; font-size: 24px; font-weight: 600; margin-bottom: 20px; }\n",
        "    .chat-area { padding: 20px; margin-bottom: 20px; }\n",
        "    .message { margin-bottom: 15px; display: flex; }\n",
        "    .message.user { justify-content: flex-end; }\n",
        "    .message.bot { justify-content: flex-start; }\n",
        "    .message-content { max-width: 70%; padding: 12px 16px; border-radius: 18px; font-size: 14px; line-height: 1.4; word-wrap: break-word; }\n",
        "    .message.user .message-content { background: #007bff; color: white; border-bottom-right-radius: 4px; }\n",
        "    .message.bot .message-content { background: #f8f9fa; color: #333; border: 1px solid #e9ecef; border-bottom-left-radius: 4px; }\n",
        "    .typing { background: #f8f9fa; color: #6c757d; padding: 12px 16px; border-radius: 18px; border-bottom-left-radius: 4px; font-style: italic; max-width: 70%; border: 1px solid #e9ecef; animation: pulse 1.5s infinite; }\n",
        "    @keyframes pulse { 0%, 100% { opacity: 0.6; } 50% { opacity: 1; } }\n",
        "    .input-area { position: fixed; bottom: 0; left: 0; right: 0; background: #000000; border-top: 1px solid #333333; padding: 15px; box-shadow: 0 -2px 4px rgba(0,0,0,0.3); z-index: 1000; }\n",
        "    .stTextInput > div > div > input { border: 1px solid #ffffff; border-radius: 25px; padding: 12px 20px; font-size: 14px; background: #333333; color: #ffffff; height: 48px; transition: border-color 0.15s ease-in-out; }\n",
        "    .stTextInput > div > div > input:focus { border-color: #ffffff; box-shadow: 0 0 0 0.2rem rgba(255,255,255,0.25); outline: none; }\n",
        "    .stTextInput > div > div > input::placeholder { color: #cccccc; }\n",
        "    .stTextInput { width: 100%; }\n",
        "    .stApp { background: #000000; }\n",
        "    .stSuccess { background: #d4edda; border: 1px solid #c3e6cb; color: #155724; border-radius: 8px; padding: 12px; margin-bottom: 20px; }\n",
        "    .stInfo { background: #d1ecf1; border: 1px solid #bee5eb; color: #0c5460; border-radius: 8px; padding: 12px; margin-bottom: 10px; }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# RAG Classes\n",
        "class RAGRetriever:\n",
        "    def __init__(self, embedding_model, faiss_index, metadata):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.faiss_index = faiss_index\n",
        "        self.metadata = metadata\n",
        "    \n",
        "    def retrieve(self, query: str, k: int = 3) -> List[Dict]:\n",
        "        query_embedding = self.embedding_model.encode([query])[0]\n",
        "        distances, indices = self.faiss_index.search(query_embedding.reshape(1, -1).astype('float32'), k)\n",
        "        retrieved_chunks = []\n",
        "        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
        "            chunk = self.metadata[\"chunks\"][idx].copy()\n",
        "            chunk[\"similarity_score\"] = 1 / (1 + distance)\n",
        "            chunk[\"rank\"] = i + 1\n",
        "            retrieved_chunks.append(chunk)\n",
        "        return retrieved_chunks\n",
        "\n",
        "class RAGSystem:\n",
        "    def __init__(self, retriever, model, tokenizer):\n",
        "        self.retriever = retriever\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def answer_question(self, question: str, k: int = 3) -> Dict:\n",
        "        retrieved_chunks = self.retriever.retrieve(question, k=k)\n",
        "        context_parts = [chunk['text'] for chunk in retrieved_chunks]\n",
        "        context = \"\\n\\n\".join(context_parts)\n",
        "        \n",
        "        rag_prompt = f\"\"\"<s>[INST] You are an expert AI assistant. Answer the question based ONLY on the provided context. If the answer is not in the context, say \"I cannot find this information in the provided context.\"\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "Please provide a comprehensive answer based on the context: [/INST]\"\"\"\n",
        "        \n",
        "        response = self.generate_response(rag_prompt)\n",
        "        return {\"question\": question, \"answer\": response, \"retrieved_chunks\": retrieved_chunks, \"context\": context}\n",
        "    \n",
        "    def generate_response(self, prompt: str, max_length: int = 512) -> str:\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "        \n",
        "        # Move inputs to the same device as the model\n",
        "        device = next(self.model.parameters()).device\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(**inputs, max_new_tokens=max_length, temperature=0.2, top_p=0.85, do_sample=True, pad_token_id=self.tokenizer.eos_token_id, eos_token_id=self.tokenizer.eos_token_id)\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        if prompt in response:\n",
        "            response = response.split(prompt)[-1].strip()\n",
        "        return response\n",
        "\n",
        "# Utility Functions\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\".join([doc.load_page(page_num).get_text() for page_num in range(len(doc))])\n",
        "    doc.close()\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'^\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\n+', '\\n', text)\n",
        "    return text.strip()\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[Dict]:\n",
        "    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
        "    chunks, current_chunk, chunk_id = [], \"\", 0\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
        "        if count_tokens(test_chunk) > chunk_size and current_chunk:\n",
        "            chunks.append({\"id\": chunk_id, \"text\": current_chunk.strip(), \"token_count\": count_tokens(current_chunk), \"char_count\": len(current_chunk)})\n",
        "            chunk_id += 1\n",
        "            overlap_text = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk\n",
        "            current_chunk = overlap_text + \" \" + sentence if overlap_text else sentence\n",
        "        else:\n",
        "            current_chunk = test_chunk\n",
        "    \n",
        "    if current_chunk.strip():\n",
        "        chunks.append({\"id\": chunk_id, \"text\": current_chunk.strip(), \"token_count\": count_tokens(current_chunk), \"char_count\": len(current_chunk)})\n",
        "    return chunks\n",
        "\n",
        "# Load Models\n",
        "@st.cache_resource\n",
        "def load_embedding_model():\n",
        "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_mistral_model():\n",
        "    import os\n",
        "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
        "    \n",
        "    # Get token from environment\n",
        "    hf_token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
        "    if not hf_token:\n",
        "        raise ValueError(\"Hugging Face token not found. Please run the authentication cell first.\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", token=hf_token)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True, token=hf_token)\n",
        "    return model, tokenizer\n",
        "\n",
        "# Main App\n",
        "st.markdown('<div class=\"main-container\">', unsafe_allow_html=True)\n",
        "st.markdown('<div class=\"header\">RAG PDF Chat System</div>', unsafe_allow_html=True)\n",
        "\n",
        "# Initialize session state\n",
        "if \"conversation\" not in st.session_state:\n",
        "    st.session_state.conversation = []\n",
        "if \"processing\" not in st.session_state:\n",
        "    st.session_state.processing = False\n",
        "if \"rag_system\" not in st.session_state:\n",
        "    st.session_state.rag_system = None\n",
        "\n",
        "# Upload PDFs\n",
        "uploaded_files = st.file_uploader(\"Upload PDF files\", type=\"pdf\", accept_multiple_files=True)\n",
        "\n",
        "if uploaded_files:\n",
        "    # Process PDFs and build RAG system\n",
        "    if st.session_state.rag_system is None:\n",
        "        with st.spinner(\"Processing PDFs and building RAG system...\"):\n",
        "            # Extract text from all PDFs\n",
        "            all_pdf_text = \"\"\n",
        "            for uploaded_file in uploaded_files:\n",
        "                with open(f\"temp_{uploaded_file.name}\", \"wb\") as f:\n",
        "                    f.write(uploaded_file.getbuffer())\n",
        "                raw_text = extract_text_from_pdf(f\"temp_{uploaded_file.name}\")\n",
        "                cleaned_text = clean_text(raw_text)\n",
        "                all_pdf_text += f\" {cleaned_text}\"\n",
        "                os.remove(f\"temp_{uploaded_file.name}\")\n",
        "            \n",
        "            # Chunk the text\n",
        "            chunks = chunk_text(all_pdf_text, chunk_size=500, overlap=50)\n",
        "            \n",
        "            # Generate embeddings\n",
        "            embedding_model = load_embedding_model()\n",
        "            texts = [chunk[\"text\"] for chunk in chunks]\n",
        "            embeddings = embedding_model.encode(texts)\n",
        "            \n",
        "            # Add embeddings to chunks\n",
        "            chunks_with_embeddings = []\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                enhanced_chunk = chunk.copy()\n",
        "                enhanced_chunk[\"embedding\"] = embeddings[i]\n",
        "                chunks_with_embeddings.append(enhanced_chunk)\n",
        "            \n",
        "            # Create FAISS index\n",
        "            embeddings_array = np.array([chunk[\"embedding\"] for chunk in chunks_with_embeddings])\n",
        "            dimension = embeddings_array.shape[1]\n",
        "            faiss_index = faiss.IndexFlatL2(dimension)\n",
        "            faiss_index.add(embeddings_array.astype('float32'))\n",
        "            \n",
        "            # Create retriever\n",
        "            retriever = RAGRetriever(embedding_model, faiss_index, {\"chunks\": chunks_with_embeddings})\n",
        "            \n",
        "            # Load Mistral model\n",
        "            mistral_model, mistral_tokenizer = load_mistral_model()\n",
        "            \n",
        "            # Create RAG system\n",
        "            st.session_state.rag_system = RAGSystem(retriever, mistral_model, mistral_tokenizer)\n",
        "    \n",
        "    # Display file information\n",
        "    st.success(f\"PDFs processed successfully!\")\n",
        "    st.info(f\"Total chunks: {len(chunks) if 'chunks' in locals() else 'N/A'}\")\n",
        "    \n",
        "    # Chat interface\n",
        "    st.markdown('<div class=\"chat-area\">', unsafe_allow_html=True)\n",
        "    \n",
        "    # Display messages\n",
        "    for turn in st.session_state.conversation:\n",
        "        if turn[\"role\"] == \"user\":\n",
        "            st.markdown(f'''<div class=\"message user\"><div class=\"message-content\">{turn[\"text\"]}</div></div>''', unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.markdown(f'''<div class=\"message bot\"><div class=\"message-content\">{turn[\"text\"]}</div></div>''', unsafe_allow_html=True)\n",
        "    \n",
        "    # Show typing indicator\n",
        "    if st.session_state.processing:\n",
        "        st.markdown('<div class=\"typing\">Bot is thinking...</div>', unsafe_allow_html=True)\n",
        "    \n",
        "    st.markdown('</div>', unsafe_allow_html=True)\n",
        "    \n",
        "    # Input area\n",
        "    st.markdown('<div class=\"input-area\">', unsafe_allow_html=True)\n",
        "    \n",
        "    # Use dynamic key to clear input field\n",
        "    if \"input_key\" not in st.session_state:\n",
        "        st.session_state.input_key = 0\n",
        "    \n",
        "    if st.session_state.get(\"input_cleared\", False):\n",
        "        st.session_state.input_key += 1\n",
        "        st.session_state.input_cleared = False\n",
        "    \n",
        "    user_input = st.text_input(\"\", placeholder=\"Ask a question about your PDFs...\", key=f\"user_input_{st.session_state.input_key}\", disabled=st.session_state.processing)\n",
        "    st.markdown('</div>', unsafe_allow_html=True)\n",
        "\n",
        "    # Handle input submission\n",
        "    if user_input and not st.session_state.processing:\n",
        "        if \"last_input\" not in st.session_state or st.session_state.last_input != user_input:\n",
        "            st.session_state.conversation.append({\"role\": \"user\", \"text\": user_input})\n",
        "            st.session_state.processing = True\n",
        "            st.session_state.last_input = user_input\n",
        "            st.session_state.input_cleared = True\n",
        "            st.rerun()\n",
        "    \n",
        "    # Process bot response\n",
        "    if st.session_state.processing and len(st.session_state.conversation) > 0 and st.session_state.conversation[-1][\"role\"] == \"user\":\n",
        "        with st.spinner(\"Generating response...\"):\n",
        "            user_question = st.session_state.conversation[-1]['text']\n",
        "            result = st.session_state.rag_system.answer_question(user_question, k=3)\n",
        "            # Only store the clean answer\n",
        "            st.session_state.conversation.append({\"role\": \"bot\", \"text\": result[\"answer\"]})\n",
        "            st.session_state.processing = False\n",
        "            st.rerun()\n",
        "\n",
        "st.markdown('</div>', unsafe_allow_html=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the Application\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Kill any existing processes\n",
        "os.system(\"pkill -f streamlit\")\n",
        "ngrok.kill()\n",
        "time.sleep(2)\n",
        "\n",
        "# Start Streamlit\n",
        "subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.headless=true\"])\n",
        "time.sleep(5)\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"RAG PDF Chat System URL:\", public_url)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
