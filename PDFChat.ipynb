{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Dependencies\n",
        "%pip install streamlit pymupdf sentence-transformers faiss-cpu transformers accelerate bitsandbytes pyngrok python-dotenv tiktoken rank-bm25 pandas plotly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Authentication\n",
        "import os\n",
        "from getpass import getpass\n",
        "from dotenv import load_dotenv\n",
        "from pyngrok import ngrok\n",
        "import huggingface_hub\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Get Hugging Face token\n",
        "hf_token = getpass(\"Enter your Hugging Face token: \")\n",
        "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = hf_token\n",
        "\n",
        "# Login to Hugging Face\n",
        "huggingface_hub.login(token=hf_token)\n",
        "\n",
        "# Get ngrok token\n",
        "ngrok_token = os.getenv(\"NGROK_AUTH_TOKEN\")\n",
        "if ngrok_token:\n",
        "    !ngrok authtoken $ngrok_token\n",
        "    print(\"Authentication setup complete!\")\n",
        "else:\n",
        "    print(\"Please set NGROK_AUTH_TOKEN in .env file\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import fitz\n",
        "import re\n",
        "import os\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import faiss\n",
        "from typing import List, Dict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import CrossEncoder\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from rank_bm25 import BM25Okapi\n",
        "import json\n",
        "import time\n",
        "\n",
        "st.set_page_config(page_title=\"RAG PDF Chat\", layout=\"wide\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    /* Remove upload zone border */\n",
        "    .upload-zone {\n",
        "        border: none;\n",
        "        padding: 0;\n",
        "        margin: 0;\n",
        "    }\n",
        "    \n",
        "    /* Remove file uploader border completely */\n",
        "    .stFileUploader > div {\n",
        "        border: none !important;\n",
        "    }\n",
        "    \n",
        "    .stFileUploader > div > div {\n",
        "        border: none !important;\n",
        "    }\n",
        "    \n",
        "    /* Chat container */\n",
        "    .chat-container {\n",
        "        max-width: 800px;\n",
        "        margin: 0 auto;\n",
        "    }\n",
        "    \n",
        "    /* User messages - right aligned */\n",
        "    .user-message {\n",
        "        background: #007bff;\n",
        "        color: white;\n",
        "        padding: 12px 18px;\n",
        "        border-radius: 18px;\n",
        "        margin: 10px 0;\n",
        "        text-align: right;\n",
        "        margin-left: 20%;\n",
        "        max-width: 80%;\n",
        "        word-wrap: break-word;\n",
        "    }\n",
        "    \n",
        "    /* Bot messages - left aligned */\n",
        "    .bot-message {\n",
        "        background: #f8f9fa;\n",
        "        color: #333;\n",
        "        padding: 12px 18px;\n",
        "        border-radius: 18px;\n",
        "        margin: 10px 0;\n",
        "        border: 1px solid #e9ecef;\n",
        "        margin-right: 20%;\n",
        "        max-width: 80%;\n",
        "        word-wrap: break-word;\n",
        "    }\n",
        "    \n",
        "    /* Context boxes - better visibility */\n",
        "    .context-box {\n",
        "        background: #f8f9fa;\n",
        "        border-left: 4px solid #28a745;\n",
        "        padding: 15px;\n",
        "        margin: 10px 0;\n",
        "        border-radius: 5px;\n",
        "        color: #333;\n",
        "        border: 1px solid #dee2e6;\n",
        "    }\n",
        "    \n",
        "    /* Citations - dark theme for visibility */\n",
        "    .citation {\n",
        "        background: #343a40;\n",
        "        color: #ffffff;\n",
        "        border: 1px solid #495057;\n",
        "        padding: 4px 8px;\n",
        "        border-radius: 12px;\n",
        "        font-size: 0.8em;\n",
        "        margin: 2px;\n",
        "        display: inline-block;\n",
        "        font-weight: 500;\n",
        "    }\n",
        "    \n",
        "    /* Professional slider styling */\n",
        "    .stSlider > div > div > div {\n",
        "        background: #f8f9fa !important;\n",
        "        border-radius: 8px !important;\n",
        "        height: 6px !important;\n",
        "        border: 1px solid #e9ecef !important;\n",
        "    }\n",
        "    \n",
        "    .stSlider > div > div > div > div {\n",
        "        background: linear-gradient(90deg, #28a745, #20c997) !important;\n",
        "        border-radius: 8px !important;\n",
        "        height: 6px !important;\n",
        "        box-shadow: none !important;\n",
        "    }\n",
        "    \n",
        "    .stSlider > div > div > div > div > div {\n",
        "        background: #ffffff !important;\n",
        "        border: 2px solid #28a745 !important;\n",
        "        border-radius: 50% !important;\n",
        "        width: 18px !important;\n",
        "        height: 18px !important;\n",
        "        box-shadow: 0 1px 3px rgba(40, 167, 69, 0.2) !important;\n",
        "        transition: all 0.2s ease !important;\n",
        "        cursor: pointer !important;\n",
        "    }\n",
        "    \n",
        "    .stSlider > div > div > div > div > div:hover {\n",
        "        transform: scale(1.05) !important;\n",
        "        box-shadow: 0 2px 4px rgba(40, 167, 69, 0.3) !important;\n",
        "        border-color: #20c997 !important;\n",
        "    }\n",
        "    \n",
        "    .stSlider > div > div > div > div > div:active {\n",
        "        transform: scale(0.95) !important;\n",
        "    }\n",
        "    \n",
        "    /* Slider track styling */\n",
        "    .stSlider > div > div > div > div > div::before {\n",
        "        content: '' !important;\n",
        "        position: absolute !important;\n",
        "        top: 50% !important;\n",
        "        left: 50% !important;\n",
        "        transform: translate(-50%, -50%) !important;\n",
        "        width: 6px !important;\n",
        "        height: 6px !important;\n",
        "        background: #28a745 !important;\n",
        "        border-radius: 50% !important;\n",
        "    }\n",
        "    \n",
        "    /* Hide the value display on sliders */\n",
        "    .stSlider > div > div > div > div > div > div {\n",
        "        display: none !important;\n",
        "    }\n",
        "    \n",
        "    /* Hide any other value displays */\n",
        "    .stSlider label {\n",
        "        display: none !important;\n",
        "    }\n",
        "    \n",
        "    .stSlider > div > div > div > div > div > div > div {\n",
        "        display: none !important;\n",
        "    }\n",
        "    \n",
        "    /* Custom checkbox colors */\n",
        "    .stCheckbox > div > label > div[data-testid=\"stMarkdownContainer\"] {\n",
        "        color: #333 !important;\n",
        "    }\n",
        "    \n",
        "    /* Better button styling */\n",
        "    .stButton > button {\n",
        "        background: #007bff;\n",
        "        color: white;\n",
        "        border: none;\n",
        "        border-radius: 6px;\n",
        "        padding: 8px 16px;\n",
        "    }\n",
        "    \n",
        "    .stButton > button:hover {\n",
        "        background: #0056b3;\n",
        "    }\n",
        "    \n",
        "    /* Sidebar improvements */\n",
        "    .css-1d391kg {\n",
        "        background: #f8f9fa;\n",
        "    }\n",
        "    \n",
        "    /* Better spacing - minimal top padding */\n",
        "    .main .block-container {\n",
        "        padding-top: 0.5rem;\n",
        "        padding-bottom: 2rem;\n",
        "    }\n",
        "    \n",
        "    /* Remove extra header padding */\n",
        "    .stApp > div:first-child {\n",
        "        padding-top: 0;\n",
        "    }\n",
        "    \n",
        "    /* Hide Streamlit header */\n",
        "    header[data-testid=\"stHeader\"] {\n",
        "        display: none;\n",
        "    }\n",
        "    \n",
        "    /* Remove any extra margins from title */\n",
        "    .stApp h1 {\n",
        "        margin-top: 0;\n",
        "        padding-top: 0;\n",
        "    }\n",
        "    \n",
        "    /* Ensure clean top spacing */\n",
        "    .stApp > div:first-child {\n",
        "        padding-top: 0 !important;\n",
        "        margin-top: 0 !important;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Model Manager for lazy loading\n",
        "class SimpleModelManager:\n",
        "    def __init__(self):\n",
        "        self._embedding_model = None\n",
        "        self._reranker = None\n",
        "        self._mistral_model = None\n",
        "        self._mistral_tokenizer = None\n",
        "    \n",
        "    @st.cache_resource\n",
        "    def get_embedding_model(_self):\n",
        "        if _self._embedding_model is None:\n",
        "            _self._embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "        return _self._embedding_model\n",
        "    \n",
        "    @st.cache_resource\n",
        "    def get_reranker(_self):\n",
        "        if _self._reranker is None:\n",
        "            _self._reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "        return _self._reranker\n",
        "    \n",
        "    @st.cache_resource\n",
        "    def get_mistral_model(_self):\n",
        "        if _self._mistral_model is None or _self._mistral_tokenizer is None:\n",
        "            hf_token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
        "            if not hf_token:\n",
        "                raise ValueError(\"Hugging Face token not found\")\n",
        "            \n",
        "            bnb_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_compute_dtype=torch.float16\n",
        "            )\n",
        "            \n",
        "            tokenizer = AutoTokenizer.from_pretrained(\n",
        "                \"mistralai/Mistral-7B-Instruct-v0.1\", \n",
        "                token=hf_token\n",
        "            )\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "            \n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "                quantization_config=bnb_config,\n",
        "                device_map=\"auto\",\n",
        "                trust_remote_code=True,\n",
        "                token=hf_token\n",
        "            )\n",
        "            \n",
        "            _self._mistral_model = model\n",
        "            _self._mistral_tokenizer = tokenizer\n",
        "        return _self._mistral_model, _self._mistral_tokenizer\n",
        "\n",
        "# Utility functions\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\".join([doc.load_page(page_num).get_text() for page_num in range(len(doc))])\n",
        "    doc.close()\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'^\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\n+', '\\n', text)\n",
        "    return text.strip()\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[Dict]:\n",
        "    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
        "    chunks, current_chunk, chunk_id = [], \"\", 0\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
        "        if count_tokens(test_chunk) > chunk_size and current_chunk:\n",
        "            chunks.append({\n",
        "                \"id\": chunk_id, \n",
        "                \"text\": current_chunk.strip(), \n",
        "                \"token_count\": count_tokens(current_chunk), \n",
        "                \"char_count\": len(current_chunk)\n",
        "            })\n",
        "            chunk_id += 1\n",
        "            overlap_text = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk\n",
        "            current_chunk = overlap_text + \" \" + sentence if overlap_text else sentence\n",
        "        else:\n",
        "            current_chunk = test_chunk\n",
        "    \n",
        "    if current_chunk.strip():\n",
        "        chunks.append({\n",
        "            \"id\": chunk_id, \n",
        "            \"text\": current_chunk.strip(), \n",
        "            \"token_count\": count_tokens(current_chunk), \n",
        "            \"char_count\": len(current_chunk)\n",
        "        })\n",
        "    return chunks\n",
        "\n",
        "# Simple Retriever\n",
        "class SimpleRetriever:\n",
        "    def __init__(self, embedding_model, faiss_index, bm25_index, metadata, reranker=None):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.faiss_index = faiss_index\n",
        "        self.bm25_index = bm25_index\n",
        "        self.metadata = metadata\n",
        "        self.reranker = reranker\n",
        "    \n",
        "    def retrieve(self, query: str, k: int = 3) -> List[Dict]:\n",
        "        # Dense retrieval\n",
        "        query_embedding = self.embedding_model.encode([query])[0]\n",
        "        distances, dense_indices = self.faiss_index.search(query_embedding.reshape(1, -1).astype('float32'), k*2)\n",
        "        \n",
        "        # Sparse retrieval\n",
        "        bm25_scores = self.bm25_index.get_scores(query.split())\n",
        "        sparse_indices = np.argsort(bm25_scores)[::-1][:k*2]\n",
        "        \n",
        "        # Combine results\n",
        "        all_indices = list(set(dense_indices[0].tolist() + sparse_indices.tolist()))\n",
        "        \n",
        "        chunks_with_scores = []\n",
        "        for idx in all_indices:\n",
        "            if idx < len(self.metadata[\"chunks\"]):\n",
        "                chunk = self.metadata[\"chunks\"][idx].copy()\n",
        "                \n",
        "                # Calculate scores\n",
        "                dense_score = 0\n",
        "                if idx in dense_indices[0]:\n",
        "                    dense_idx = np.where(dense_indices[0] == idx)[0]\n",
        "                    if len(dense_idx) > 0:\n",
        "                        dense_score = 1 / (1 + distances[0][dense_idx[0]])\n",
        "                \n",
        "                sparse_score = bm25_scores[idx] / (np.max(bm25_scores) + 1e-8)\n",
        "                combined_score = 0.6 * dense_score + 0.4 * sparse_score\n",
        "                \n",
        "                chunk[\"dense_score\"] = dense_score\n",
        "                chunk[\"sparse_score\"] = sparse_score\n",
        "                chunk[\"combined_score\"] = combined_score\n",
        "                chunks_with_scores.append(chunk)\n",
        "        \n",
        "        # Sort by combined score\n",
        "        chunks_with_scores.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n",
        "        \n",
        "        # Rerank if available\n",
        "        if self.reranker and len(chunks_with_scores) > k:\n",
        "            query_chunk_pairs = [(query, chunk[\"text\"]) for chunk in chunks_with_scores[:k*2]]\n",
        "            rerank_scores = self.reranker.predict(query_chunk_pairs)\n",
        "            \n",
        "            for i, chunk in enumerate(chunks_with_scores[:k*2]):\n",
        "                chunk[\"rerank_score\"] = rerank_scores[i]\n",
        "                chunk[\"final_score\"] = 0.7 * chunk[\"combined_score\"] + 0.3 * rerank_scores[i]\n",
        "        else:\n",
        "            for chunk in chunks_with_scores:\n",
        "                chunk[\"final_score\"] = chunk[\"combined_score\"]\n",
        "        \n",
        "        chunks_with_scores.sort(key=lambda x: x[\"final_score\"], reverse=True)\n",
        "        \n",
        "        # Add ranking\n",
        "        for i, chunk in enumerate(chunks_with_scores[:k]):\n",
        "            chunk[\"rank\"] = i + 1\n",
        "        \n",
        "        return chunks_with_scores[:k]\n",
        "\n",
        "# Simple RAG System\n",
        "class SimpleRAGSystem:\n",
        "    def __init__(self, retriever, model, tokenizer):\n",
        "        self.retriever = retriever\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def answer_question(self, question: str, k: int = 3) -> Dict:\n",
        "        retrieved_chunks = self.retriever.retrieve(question, k=k)\n",
        "        \n",
        "        # Create context with citations\n",
        "        context_parts = []\n",
        "        for i, chunk in enumerate(retrieved_chunks):\n",
        "            citation = f\"[{i+1}]\"\n",
        "            context_parts.append(f\"{citation} {chunk['text']}\")\n",
        "        context = \"\\n\\n\".join(context_parts)\n",
        "        \n",
        "        # Enhanced prompt\n",
        "        rag_prompt = f\"\"\"<s>[INST] You are an expert AI assistant. Answer the question based ONLY on the provided context. Use citations [1], [2], etc. to reference specific sources. If the answer is not in the context, say \"I cannot find this information in the provided context.\"\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "Please provide a comprehensive answer with proper citations based on the context: [/INST]\"\"\"\n",
        "        \n",
        "        response = self.generate_response(rag_prompt)\n",
        "        return {\n",
        "            \"question\": question, \n",
        "            \"answer\": response, \n",
        "            \"retrieved_chunks\": retrieved_chunks, \n",
        "            \"context\": context\n",
        "        }\n",
        "    \n",
        "    def generate_response(self, prompt: str, max_length: int = 512) -> str:\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "        \n",
        "        # Move to same device as model\n",
        "        device = next(self.model.parameters()).device\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs, \n",
        "                max_new_tokens=max_length, \n",
        "                temperature=0.2, \n",
        "                top_p=0.85, \n",
        "                do_sample=True, \n",
        "                pad_token_id=self.tokenizer.eos_token_id, \n",
        "                eos_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Clean up the response to show only the actual answer\n",
        "        if prompt in response:\n",
        "            response = response.split(prompt)[-1].strip()\n",
        "        \n",
        "        # Remove any remaining prompt artifacts\n",
        "        if \"[/INST]\" in response:\n",
        "            response = response.split(\"[/INST]\")[-1].strip()\n",
        "        \n",
        "        # Remove any remaining context or question references\n",
        "        if \"CONTEXT:\" in response:\n",
        "            response = response.split(\"CONTEXT:\")[0].strip()\n",
        "        if \"QUESTION:\" in response:\n",
        "            response = response.split(\"QUESTION:\")[0].strip()\n",
        "        \n",
        "        # Clean up any remaining artifacts\n",
        "        response = response.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").strip()\n",
        "        \n",
        "        return response\n",
        "\n",
        "# Initialize model manager\n",
        "model_manager = SimpleModelManager()\n",
        "\n",
        "# Main UI\n",
        "st.title(\"RAG PDF Chat System\")\n",
        "st.markdown(\"Upload PDFs and ask questions about their content\")\n",
        "\n",
        "# Sidebar for settings\n",
        "with st.sidebar:\n",
        "    st.header(\"Settings\")\n",
        "    \n",
        "    # Model selection\n",
        "    model_type = st.selectbox(\n",
        "        \"Select Model\",\n",
        "        [\"Mistral-7B\", \"FLAN-T5\", \"GPT-2\"],\n",
        "        help=\"Choose the language model for generation\"\n",
        "    )\n",
        "    \n",
        "    # Retriever selection\n",
        "    retriever_type = st.selectbox(\n",
        "        \"Select Retriever\",\n",
        "        [\"Hybrid (Dense + Sparse)\", \"Dense Only\", \"Sparse Only\"],\n",
        "        help=\"Choose the retrieval method\"\n",
        "    )\n",
        "    \n",
        "    # Number of chunks\n",
        "    num_chunks = st.slider(\n",
        "        \"Number of chunks to retrieve\",\n",
        "        min_value=1,\n",
        "        max_value=10,\n",
        "        value=3,\n",
        "        help=\"More chunks = more context but slower\"\n",
        "    )\n",
        "    \n",
        "    # Show advanced options\n",
        "    show_advanced = st.checkbox(\"Show Advanced Options\")\n",
        "    if show_advanced:\n",
        "        chunk_size = st.slider(\"Chunk Size\", 200, 1000, 500)\n",
        "        temperature = st.slider(\"Temperature\", 0.1, 1.0, 0.2)\n",
        "        max_tokens = st.slider(\"Max Tokens\", 100, 1000, 512)\n",
        "\n",
        "# Upload zone\n",
        "uploaded_files = st.file_uploader(\n",
        "    \"Upload PDF files\", \n",
        "    type=\"pdf\", \n",
        "    accept_multiple_files=True,\n",
        "    help=\"Upload one or more PDF files to analyze\"\n",
        ")\n",
        "\n",
        "# Initialize session state\n",
        "if \"conversation\" not in st.session_state:\n",
        "    st.session_state.conversation = []\n",
        "if \"rag_system\" not in st.session_state:\n",
        "    st.session_state.rag_system = None\n",
        "if \"processing\" not in st.session_state:\n",
        "    st.session_state.processing = False\n",
        "\n",
        "# Process uploaded files\n",
        "if uploaded_files:\n",
        "    if st.session_state.rag_system is None:\n",
        "        st.info(\"Getting model ready...\")\n",
        "        try:\n",
        "            # Extract text from all PDFs\n",
        "            all_pdf_text = \"\"\n",
        "            for uploaded_file in uploaded_files:\n",
        "                with open(f\"temp_{uploaded_file.name}\", \"wb\") as f:\n",
        "                    f.write(uploaded_file.getbuffer())\n",
        "                raw_text = extract_text_from_pdf(f\"temp_{uploaded_file.name}\")\n",
        "                cleaned_text = clean_text(raw_text)\n",
        "                all_pdf_text += f\" {cleaned_text}\"\n",
        "                os.remove(f\"temp_{uploaded_file.name}\")\n",
        "            \n",
        "            # Chunk the text\n",
        "            chunks = chunk_text(all_pdf_text, chunk_size=500, overlap=50)\n",
        "            \n",
        "            # Generate embeddings\n",
        "            embedding_model = model_manager.get_embedding_model()\n",
        "            texts = [chunk[\"text\"] for chunk in chunks]\n",
        "            embeddings = embedding_model.encode(texts)\n",
        "            \n",
        "            # Add embeddings to chunks\n",
        "            chunks_with_embeddings = []\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                enhanced_chunk = chunk.copy()\n",
        "                enhanced_chunk[\"embedding\"] = embeddings[i]\n",
        "                chunks_with_embeddings.append(enhanced_chunk)\n",
        "            \n",
        "            # Create FAISS index\n",
        "            embeddings_array = np.array([chunk[\"embedding\"] for chunk in chunks_with_embeddings])\n",
        "            dimension = embeddings_array.shape[1]\n",
        "            faiss_index = faiss.IndexFlatL2(dimension)\n",
        "            faiss_index.add(embeddings_array.astype('float32'))\n",
        "            \n",
        "            # Create BM25 index\n",
        "            tokenized_texts = [text.split() for text in texts]\n",
        "            bm25_index = BM25Okapi(tokenized_texts)\n",
        "            \n",
        "            # Load reranker\n",
        "            reranker = model_manager.get_reranker()\n",
        "            \n",
        "            # Create retriever\n",
        "            retriever = SimpleRetriever(embedding_model, faiss_index, bm25_index, {\"chunks\": chunks_with_embeddings}, reranker)\n",
        "            \n",
        "            # Load Mistral model\n",
        "            mistral_model, mistral_tokenizer = model_manager.get_mistral_model()\n",
        "            \n",
        "            # Create RAG system\n",
        "            st.session_state.rag_system = SimpleRAGSystem(retriever, mistral_model, mistral_tokenizer)\n",
        "            \n",
        "            st.success(f\"Successfully processed {len(uploaded_files)} PDF(s) with {len(chunks)} chunks!\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            st.error(f\"Error processing PDFs: {str(e)}\")\n",
        "            st.stop()\n",
        "    \n",
        "    # Chat interface\n",
        "    st.markdown('<div class=\"chat-container\">', unsafe_allow_html=True)\n",
        "    \n",
        "    # Display conversation history\n",
        "    for i, turn in enumerate(st.session_state.conversation):\n",
        "        if turn[\"role\"] == \"user\":\n",
        "            st.markdown(f'<div class=\"user-message\">{turn[\"text\"]}</div>', unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.markdown(f'<div class=\"bot-message\">{turn[\"text\"]}</div>', unsafe_allow_html=True)\n",
        "            \n",
        "            # Show retrieved context if available\n",
        "            if \"context\" in turn and turn[\"context\"]:\n",
        "                with st.expander(f\"Retrieved Context (Turn {i+1})\", expanded=False):\n",
        "                    contexts = turn[\"context\"].split(\"\\n\\n\")\n",
        "                    for j, context in enumerate(contexts):\n",
        "                        if context.strip():\n",
        "                            st.markdown(f'<div class=\"context-box\"><strong>Source {j+1}:</strong><br>{context}</div>', unsafe_allow_html=True)\n",
        "    \n",
        "    # Show typing indicator\n",
        "    if st.session_state.processing:\n",
        "        st.markdown('<div class=\"bot-message\">Thinking...</div>', unsafe_allow_html=True)\n",
        "    \n",
        "    # Question input\n",
        "    st.markdown(\"### Ask a Question\")\n",
        "    user_input = st.text_input(\n",
        "        \"\", \n",
        "        placeholder=\"Ask a question about your PDFs...\", \n",
        "        disabled=st.session_state.processing,\n",
        "        key=\"user_input\"\n",
        "    )\n",
        "    \n",
        "    # Handle input submission\n",
        "    if user_input and not st.session_state.processing:\n",
        "        if \"last_input\" not in st.session_state or st.session_state.last_input != user_input:\n",
        "            # Add user message\n",
        "            st.session_state.conversation.append({\"role\": \"user\", \"text\": user_input})\n",
        "            st.session_state.processing = True\n",
        "            st.session_state.last_input = user_input\n",
        "            st.rerun()\n",
        "    \n",
        "    # Process bot response\n",
        "    if st.session_state.processing and len(st.session_state.conversation) > 0 and st.session_state.conversation[-1][\"role\"] == \"user\":\n",
        "        try:\n",
        "            user_question = st.session_state.conversation[-1]['text']\n",
        "            \n",
        "            # Generate response first\n",
        "            result = st.session_state.rag_system.answer_question(user_question, k=num_chunks)\n",
        "            \n",
        "            # Clear the \"Thinking...\" message by updating processing state\n",
        "            st.session_state.processing = False\n",
        "            \n",
        "            # Add the response to conversation\n",
        "            st.session_state.conversation.append({\n",
        "                \"role\": \"bot\", \n",
        "                \"text\": result[\"answer\"],\n",
        "                \"context\": result[\"context\"],\n",
        "                \"retrieved_chunks\": result[\"retrieved_chunks\"]\n",
        "            })\n",
        "            \n",
        "            # Rerun to show the response\n",
        "            st.rerun()\n",
        "            \n",
        "        except Exception as e:\n",
        "            st.error(f\"Error generating response: {str(e)}\")\n",
        "            st.session_state.conversation.append({\n",
        "                \"role\": \"bot\", \n",
        "                \"text\": \"I apologize, but I encountered an error while processing your question. Please try again.\"\n",
        "            })\n",
        "            st.session_state.processing = False\n",
        "            st.rerun()\n",
        "    \n",
        "    st.markdown('</div>', unsafe_allow_html=True)\n",
        "\n",
        "else:\n",
        "    st.info(\"Please upload PDF files to get started\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the Application\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Kill any existing processes\n",
        "os.system(\"pkill -f streamlit\")\n",
        "ngrok.kill()\n",
        "time.sleep(2)\n",
        "\n",
        "# Start Streamlit\n",
        "subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.headless=true\"])\n",
        "time.sleep(5)\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"RAG PDF Chat System URL:\", public_url)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
