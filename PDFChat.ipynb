{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chat with PDFs Application\n",
        "\n",
        "This notebook allows you to upload **ANY type of PDF** and ask questions about its content using AI models.\n",
        "\n",
        "**Works with all PDF types:**\n",
        "- üìÑ Utility bills, invoices, receipts\n",
        "- üìã Contracts, legal documents\n",
        "- üìä Reports, presentations, spreadsheets\n",
        "- üìö Books, research papers, articles\n",
        "- üè• Medical records, prescriptions\n",
        "- üìù Forms, applications, certificates\n",
        "- üíº Business documents, memos\n",
        "- And literally any other PDF!\n",
        "\n",
        "**Features:**\n",
        "- Upload and process multiple PDF files of any type\n",
        "- Extract and analyze text from any domain\n",
        "- Ask questions in natural language\n",
        "- Get instant answers with source citations\n",
        "- Works with GPT-3.5 or open-source models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Required Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q langchain langchain-community langchain-openai\n",
        "%pip install -q pypdf chromadb\n",
        "%pip install -q sentence-transformers\n",
        "%pip install -q openai tiktoken\n",
        "%pip install -q transformers accelerate bitsandbytes\n",
        "%pip install -q faiss-cpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from google.colab import files\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Configuration\n",
        "\n",
        "Choose which model you want to use:\n",
        "- **Option 1:** OpenAI GPT-3.5-turbo (requires API key)\n",
        "- **Option 2:** Open-source models from HuggingFace (free, but may be slower)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "USE_OPENAI = False  # Set to True if you want to use GPT-3.5, False for open-source\n",
        "\n",
        "if USE_OPENAI:\n",
        "    # If using OpenAI, enter your API key\n",
        "    OPENAI_API_KEY = input(\"Enter your OpenAI API key: \")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "    print(\"‚úÖ OpenAI API key set!\")\n",
        "else:\n",
        "    # If using HuggingFace, you can optionally provide a token for gated models\n",
        "    print(\"Using open-source models from HuggingFace (no API key needed)\")\n",
        "    # Uncomment below if you want to use gated models\n",
        "    # HUGGINGFACE_TOKEN = input(\"Enter your HuggingFace token (optional): \")\n",
        "    # os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACE_TOKEN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Upload PDF Files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload PDF files\n",
        "print(\"üìÅ Please upload your PDF files...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Save uploaded files\n",
        "pdf_files = []\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.pdf'):\n",
        "        pdf_files.append(filename)\n",
        "        print(f\"‚úÖ Uploaded: {filename}\")\n",
        "\n",
        "print(f\"\\nüìö Total PDF files uploaded: {len(pdf_files)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Process PDFs and Create Vector Store\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and split PDFs\n",
        "print(\"üìñ Loading and processing PDFs...\")\n",
        "\n",
        "all_documents = []\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "    loader = PyPDFLoader(pdf_file)\n",
        "    documents = loader.load()\n",
        "    all_documents.extend(documents)\n",
        "    print(f\"‚úÖ Loaded {len(documents)} pages from {pdf_file}\")\n",
        "\n",
        "print(f\"\\nüìÑ Total pages loaded: {len(all_documents)}\")\n",
        "\n",
        "# Split documents into chunks\n",
        "print(\"\\n‚úÇÔ∏è Splitting documents into chunks...\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_documents(all_documents)\n",
        "print(f\"‚úÖ Created {len(chunks)} text chunks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create embeddings and vector store\n",
        "print(\"\\nüî¢ Creating embeddings (this may take a few minutes)...\")\n",
        "\n",
        "# Using HuggingFace embeddings (free and works well)\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'}\n",
        ")\n",
        "\n",
        "# Create FAISS vector store\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "print(\"‚úÖ Vector store created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Setup Question-Answering Chain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create prompt template (works for ANY document type)\n",
        "prompt_template = \"\"\"You are a helpful AI assistant that answers questions about documents.\n",
        "Use the following pieces of context from the uploaded PDF(s) to answer the question.\n",
        "The document could be anything: a bill, invoice, contract, report, article, receipt, or any other type.\n",
        "\n",
        "Be specific and extract exact information when available (numbers, dates, names, amounts, etc.).\n",
        "If you don't know the answer based on the context, just say that you don't know - don't make up information.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, \n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Setup the language model\n",
        "if USE_OPENAI:\n",
        "    print(\"ü§ñ Setting up GPT-3.5-turbo...\")\n",
        "    llm = ChatOpenAI(\n",
        "        model_name=\"gpt-3.5-turbo\",\n",
        "        temperature=0.7,\n",
        "    )\n",
        "else:\n",
        "    print(\"ü§ñ Setting up open-source model (Google FLAN-T5-XXL)...\")\n",
        "    # Using FLAN-T5 which is good for question answering\n",
        "    llm = HuggingFaceHub(\n",
        "        repo_id=\"google/flan-t5-xxl\",\n",
        "        model_kwargs={\"temperature\": 0.7, \"max_length\": 512}\n",
        "    )\n",
        "\n",
        "# Create QA chain with more chunks for better coverage\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4}),  # Retrieve more chunks for better accuracy\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": PROMPT}\n",
        ")\n",
        "\n",
        "print(\"‚úÖ QA chain ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Chat with Your PDFs!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_question(question):\n",
        "    \"\"\"\n",
        "    Ask a question about the uploaded PDFs\n",
        "    \"\"\"\n",
        "    print(f\"\\n‚ùì Question: {question}\")\n",
        "    print(\"\\nüîç Searching through documents...\\n\")\n",
        "    \n",
        "    result = qa_chain.invoke({\"query\": question})\n",
        "    \n",
        "    print(\"üí° Answer:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(result[\"result\"])\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Show source documents\n",
        "    print(\"\\nüìö Sources:\")\n",
        "    for i, doc in enumerate(result[\"source_documents\"], 1):\n",
        "        source = doc.metadata.get('source', 'Unknown')\n",
        "        page = doc.metadata.get('page', 'Unknown')\n",
        "        print(f\"  [{i}] {source} (Page {page + 1})\")\n",
        "    \n",
        "    return result[\"result\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage - Ask your first question!\n",
        "question = input(\"\\nüó£Ô∏è Enter your question: \")\n",
        "answer = ask_question(question)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Interactive Chat Loop (Optional)\n",
        "\n",
        "Run this cell for a continuous chat experience. Type 'quit' or 'exit' to stop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üí¨ Interactive Chat Mode\")\n",
        "print(\"Type 'quit' or 'exit' to stop\\n\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "while True:\n",
        "    question = input(\"\\nüó£Ô∏è You: \")\n",
        "    \n",
        "    if question.lower() in ['quit', 'exit', 'q']:\n",
        "        print(\"\\nüëã Thanks for chatting! Goodbye!\")\n",
        "        break\n",
        "    \n",
        "    if not question.strip():\n",
        "        print(\"‚ö†Ô∏è Please enter a valid question.\")\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        answer = ask_question(question)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error: {str(e)}\")\n",
        "        print(\"Please try rephrasing your question.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Features\n",
        "\n",
        "### Get Similar Documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_similar_content(query, k=5):\n",
        "    \"\"\"\n",
        "    Search for similar content in the PDFs\n",
        "    \"\"\"\n",
        "    print(f\"\\nüîç Searching for content related to: '{query}'\\n\")\n",
        "    \n",
        "    docs = vectorstore.similarity_search(query, k=k)\n",
        "    \n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        print(f\"\\n[{i}] Source: {doc.metadata.get('source', 'Unknown')} (Page {doc.metadata.get('page', 'Unknown') + 1})\")\n",
        "        print(\"-\" * 80)\n",
        "        print(doc.page_content[:300] + \"...\" if len(doc.page_content) > 300 else doc.page_content)\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "# Example usage\n",
        "# search_similar_content(\"your search term here\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Vector Store (Optional)\n",
        "\n",
        "Save the vector store so you don't need to reprocess PDFs next time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save vector store\n",
        "vectorstore.save_local(\"pdf_vectorstore\")\n",
        "print(\"‚úÖ Vector store saved!\")\n",
        "\n",
        "# To load it later:\n",
        "# vectorstore = FAISS.load_local(\"pdf_vectorstore\", embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tips for Better Results\n",
        "\n",
        "1. **Be specific with your questions**: Instead of \"What is this about?\", try \"What are the main conclusions about X?\"\n",
        "\n",
        "2. **Ask follow-up questions**: Build on previous answers for deeper understanding\n",
        "\n",
        "3. **Model Selection**:\n",
        "   - **GPT-3.5**: Better quality answers, faster, but requires API key and costs money\n",
        "   - **FLAN-T5**: Free, open-source, good for factual questions, but may be slower\n",
        "\n",
        "4. **For longer or more complex PDFs**: Consider increasing the `k` parameter in the retriever to search more chunks\n",
        "\n",
        "5. **If answers are not good**: Try adjusting:\n",
        "   - `chunk_size`: Larger chunks (1500-2000) for more context\n",
        "   - `chunk_overlap`: More overlap (300-400) for better continuity\n",
        "   - `temperature`: Lower (0.3-0.5) for more focused answers\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
