{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Dependencies\n",
        "%pip install streamlit pymupdf sentence-transformers faiss-cpu transformers accelerate bitsandbytes pyngrok python-dotenv tiktoken rank-bm25 pandas plotly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Authentication\n",
        "import os\n",
        "from getpass import getpass\n",
        "from dotenv import load_dotenv\n",
        "from pyngrok import ngrok\n",
        "import huggingface_hub\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Get Hugging Face token\n",
        "hf_token = getpass(\"Enter your Hugging Face token: \")\n",
        "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = hf_token\n",
        "\n",
        "# Login to Hugging Face\n",
        "huggingface_hub.login(token=hf_token)\n",
        "\n",
        "# Get ngrok token\n",
        "ngrok_token = os.getenv(\"NGROK_AUTH_TOKEN\")\n",
        "if ngrok_token:\n",
        "    !ngrok authtoken $ngrok_token\n",
        "    print(\"Authentication setup complete!\")\n",
        "else:\n",
        "    print(\"Please set NGROK_AUTH_TOKEN in .env file\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import fitz\n",
        "import re\n",
        "import os\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import faiss\n",
        "from typing import List, Dict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sentence_transformers import CrossEncoder\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from rank_bm25 import BM25Okapi\n",
        "import json\n",
        "\n",
        "st.set_page_config(page_title=\"PDF Chat\", layout=\"wide\")\n",
        "\n",
        "# CSS Styling\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    header[data-testid=\"stHeader\"] { display: none; }\n",
        "    .stApp > div:first-child { padding-top: 0; }\n",
        "    .main-container { max-width: 800px; margin: 0 auto; padding: 20px; padding-bottom: 100px; }\n",
        "    .header { text-align: center; color: #ffffff; font-size: 24px; font-weight: 600; margin-bottom: 20px; }\n",
        "    .chat-area { padding: 20px; margin-bottom: 20px; }\n",
        "    .message { margin-bottom: 15px; display: flex; }\n",
        "    .message.user { justify-content: flex-end; }\n",
        "    .message.bot { justify-content: flex-start; }\n",
        "    .message-content { max-width: 70%; padding: 12px 16px; border-radius: 18px; font-size: 14px; line-height: 1.4; word-wrap: break-word; }\n",
        "    .message.user .message-content { background: #007bff; color: white; border-bottom-right-radius: 4px; }\n",
        "    .message.bot .message-content { background: #f8f9fa; color: #333; border: 1px solid #e9ecef; border-bottom-left-radius: 4px; }\n",
        "    .typing { background: #f8f9fa; color: #6c757d; padding: 12px 16px; border-radius: 18px; border-bottom-left-radius: 4px; font-style: italic; max-width: 70%; border: 1px solid #e9ecef; animation: pulse 1.5s infinite; }\n",
        "    @keyframes pulse { 0%, 100% { opacity: 0.6; } 50% { opacity: 1; } }\n",
        "    .input-area { position: fixed; bottom: 0; left: 0; right: 0; background: #000000; border-top: 1px solid #333333; padding: 15px; box-shadow: 0 -2px 4px rgba(0,0,0,0.3); z-index: 1000; }\n",
        "    .stTextInput > div > div > input { border: 1px solid #ffffff; border-radius: 25px; padding: 12px 20px; font-size: 14px; background: #333333; color: #ffffff; height: 48px; transition: border-color 0.15s ease-in-out; }\n",
        "    .stTextInput > div > div > input:focus { border-color: #ffffff; box-shadow: 0 0 0 0.2rem rgba(255,255,255,0.25); outline: none; }\n",
        "    .stTextInput > div > div > input::placeholder { color: #cccccc; }\n",
        "    .stTextInput { width: 100%; }\n",
        "    .stApp { background: #000000; }\n",
        "    .stSuccess { background: #d4edda; border: 1px solid #c3e6cb; color: #155724; border-radius: 8px; padding: 12px; margin-bottom: 20px; }\n",
        "    .stInfo { background: #d1ecf1; border: 1px solid #bee5eb; color: #0c5460; border-radius: 8px; padding: 12px; margin-bottom: 10px; }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Enhanced RAG Classes\n",
        "class HybridRetriever:\n",
        "    def __init__(self, embedding_model, faiss_index, bm25_index, metadata, reranker=None):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.faiss_index = faiss_index\n",
        "        self.bm25_index = bm25_index\n",
        "        self.metadata = metadata\n",
        "        self.reranker = reranker\n",
        "    \n",
        "    def retrieve(self, query: str, k: int = 5) -> List[Dict]:\n",
        "        # Dense retrieval (semantic)\n",
        "        query_embedding = self.embedding_model.encode([query])[0]\n",
        "        distances, dense_indices = self.faiss_index.search(query_embedding.reshape(1, -1).astype('float32'), k*2)\n",
        "        \n",
        "        # Sparse retrieval (BM25)\n",
        "        bm25_scores = self.bm25_index.get_scores(query.split())\n",
        "        sparse_indices = np.argsort(bm25_scores)[::-1][:k*2]\n",
        "        \n",
        "        # Combine and deduplicate\n",
        "        all_indices = list(set(dense_indices[0].tolist() + sparse_indices.tolist()))\n",
        "        \n",
        "        # Get chunks with scores\n",
        "        chunks_with_scores = []\n",
        "        for idx in all_indices:\n",
        "            if idx < len(self.metadata[\"chunks\"]):\n",
        "                chunk = self.metadata[\"chunks\"][idx].copy()\n",
        "                # Dense score\n",
        "                dense_score = 0\n",
        "                if idx in dense_indices[0]:\n",
        "                    dense_idx = np.where(dense_indices[0] == idx)[0]\n",
        "                    if len(dense_idx) > 0:\n",
        "                        dense_score = 1 / (1 + distances[0][dense_idx[0]])\n",
        "                \n",
        "                # Sparse score\n",
        "                sparse_score = bm25_scores[idx] / (np.max(bm25_scores) + 1e-8)\n",
        "                \n",
        "                # Combined score\n",
        "                combined_score = 0.6 * dense_score + 0.4 * sparse_score\n",
        "                \n",
        "                chunk[\"dense_score\"] = dense_score\n",
        "                chunk[\"sparse_score\"] = sparse_score\n",
        "                chunk[\"combined_score\"] = combined_score\n",
        "                chunks_with_scores.append(chunk)\n",
        "        \n",
        "        # Sort by combined score\n",
        "        chunks_with_scores.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n",
        "        \n",
        "        # Rerank if reranker is available\n",
        "        if self.reranker and len(chunks_with_scores) > k:\n",
        "            query_chunk_pairs = [(query, chunk[\"text\"]) for chunk in chunks_with_scores[:k*2]]\n",
        "            rerank_scores = self.reranker.predict(query_chunk_pairs)\n",
        "            \n",
        "            for i, chunk in enumerate(chunks_with_scores[:k*2]):\n",
        "                chunk[\"rerank_score\"] = rerank_scores[i]\n",
        "                chunk[\"final_score\"] = 0.7 * chunk[\"combined_score\"] + 0.3 * rerank_scores[i]\n",
        "            else:\n",
        "                for chunk in chunks_with_scores:\n",
        "                    chunk[\"final_score\"] = chunk[\"combined_score\"]\n",
        "            \n",
        "            chunks_with_scores.sort(key=lambda x: x[\"final_score\"], reverse=True)\n",
        "        \n",
        "        # Return top k with ranking\n",
        "        for i, chunk in enumerate(chunks_with_scores[:k]):\n",
        "            chunk[\"rank\"] = i + 1\n",
        "        \n",
        "        return chunks_with_scores[:k]\n",
        "\n",
        "class EnhancedRAGSystem:\n",
        "    def __init__(self, retriever, model, tokenizer):\n",
        "        self.retriever = retriever\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def answer_question(self, question: str, k: int = 5) -> Dict:\n",
        "        retrieved_chunks = self.retriever.retrieve(question, k=k)\n",
        "        \n",
        "        # Create context with citations\n",
        "        context_parts = []\n",
        "        for i, chunk in enumerate(retrieved_chunks):\n",
        "            citation = f\"[{i+1}]\"\n",
        "            context_parts.append(f\"{citation} {chunk['text']}\")\n",
        "        context = \"\\n\\n\".join(context_parts)\n",
        "        \n",
        "        # Enhanced prompt with citation instructions\n",
        "        rag_prompt = f\"\"\"<s>[INST] You are an expert AI assistant. Answer the question based ONLY on the provided context. Use citations [1], [2], etc. to reference specific sources. If the answer is not in the context, say \"I cannot find this information in the provided context.\"\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "Please provide a comprehensive answer with proper citations based on the context: [/INST]\"\"\"\n",
        "        \n",
        "        response = self.generate_response(rag_prompt)\n",
        "        return {\n",
        "            \"question\": question, \n",
        "            \"answer\": response, \n",
        "            \"retrieved_chunks\": retrieved_chunks, \n",
        "            \"context\": context,\n",
        "            \"retrieval_scores\": {f\"chunk_{i+1}\": {\n",
        "                \"dense_score\": chunk.get(\"dense_score\", 0),\n",
        "                \"sparse_score\": chunk.get(\"sparse_score\", 0),\n",
        "                \"combined_score\": chunk.get(\"combined_score\", 0),\n",
        "                \"final_score\": chunk.get(\"final_score\", chunk.get(\"combined_score\", 0))\n",
        "            } for i, chunk in enumerate(retrieved_chunks)}\n",
        "        }\n",
        "    \n",
        "    def generate_response(self, prompt: str, max_length: int = 512) -> str:\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "        \n",
        "        # Move inputs to the same device as the model\n",
        "        device = next(self.model.parameters()).device\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(**inputs, max_new_tokens=max_length, temperature=0.2, top_p=0.85, do_sample=True, pad_token_id=self.tokenizer.eos_token_id, eos_token_id=self.tokenizer.eos_token_id)\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        if prompt in response:\n",
        "            response = response.split(prompt)[-1].strip()\n",
        "        return response\n",
        "\n",
        "# Utility Functions\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\".join([doc.load_page(page_num).get_text() for page_num in range(len(doc))])\n",
        "    doc.close()\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'^\\d+\\s*$', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\n+', '\\n', text)\n",
        "    return text.strip()\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[Dict]:\n",
        "    sentences = [s.strip() for s in re.split(r'[.!?]+', text) if s.strip()]\n",
        "    chunks, current_chunk, chunk_id = [], \"\", 0\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        test_chunk = current_chunk + \" \" + sentence if current_chunk else sentence\n",
        "        if count_tokens(test_chunk) > chunk_size and current_chunk:\n",
        "            chunks.append({\"id\": chunk_id, \"text\": current_chunk.strip(), \"token_count\": count_tokens(current_chunk), \"char_count\": len(current_chunk)})\n",
        "            chunk_id += 1\n",
        "            overlap_text = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk\n",
        "            current_chunk = overlap_text + \" \" + sentence if overlap_text else sentence\n",
        "        else:\n",
        "            current_chunk = test_chunk\n",
        "    \n",
        "    if current_chunk.strip():\n",
        "        chunks.append({\"id\": chunk_id, \"text\": current_chunk.strip(), \"token_count\": count_tokens(current_chunk), \"char_count\": len(current_chunk)})\n",
        "    return chunks\n",
        "\n",
        "# Load Models\n",
        "@st.cache_resource\n",
        "def load_embedding_model():\n",
        "    return SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_reranker():\n",
        "    return CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "@st.cache_resource\n",
        "def load_mistral_model():\n",
        "    import os\n",
        "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
        "    \n",
        "    # Get token from environment\n",
        "    hf_token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
        "    if not hf_token:\n",
        "        raise ValueError(\"Hugging Face token not found. Please run the authentication cell first.\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", token=hf_token)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True, token=hf_token)\n",
        "    return model, tokenizer\n",
        "\n",
        "# Main App\n",
        "st.markdown('<div class=\"main-container\">', unsafe_allow_html=True)\n",
        "st.markdown('<div class=\"header\">RAG PDF Chat System</div>', unsafe_allow_html=True)\n",
        "\n",
        "# Initialize session state\n",
        "if \"conversation\" not in st.session_state:\n",
        "    st.session_state.conversation = []\n",
        "if \"processing\" not in st.session_state:\n",
        "    st.session_state.processing = False\n",
        "if \"rag_system\" not in st.session_state:\n",
        "    st.session_state.rag_system = None\n",
        "\n",
        "# Upload PDFs\n",
        "uploaded_files = st.file_uploader(\"Upload PDF files\", type=\"pdf\", accept_multiple_files=True)\n",
        "\n",
        "if uploaded_files:\n",
        "    # Process PDFs and build RAG system\n",
        "    if st.session_state.rag_system is None:\n",
        "        with st.spinner(\"Processing PDFs and building RAG system...\"):\n",
        "            # Extract text from all PDFs\n",
        "            all_pdf_text = \"\"\n",
        "            for uploaded_file in uploaded_files:\n",
        "                with open(f\"temp_{uploaded_file.name}\", \"wb\") as f:\n",
        "                    f.write(uploaded_file.getbuffer())\n",
        "                raw_text = extract_text_from_pdf(f\"temp_{uploaded_file.name}\")\n",
        "                cleaned_text = clean_text(raw_text)\n",
        "                all_pdf_text += f\" {cleaned_text}\"\n",
        "                os.remove(f\"temp_{uploaded_file.name}\")\n",
        "            \n",
        "            # Chunk the text\n",
        "            chunks = chunk_text(all_pdf_text, chunk_size=500, overlap=50)\n",
        "            \n",
        "            # Generate embeddings\n",
        "            embedding_model = load_embedding_model()\n",
        "            texts = [chunk[\"text\"] for chunk in chunks]\n",
        "            embeddings = embedding_model.encode(texts)\n",
        "            \n",
        "            # Add embeddings to chunks\n",
        "            chunks_with_embeddings = []\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                enhanced_chunk = chunk.copy()\n",
        "                enhanced_chunk[\"embedding\"] = embeddings[i]\n",
        "                chunks_with_embeddings.append(enhanced_chunk)\n",
        "            \n",
        "            # Create FAISS index\n",
        "            embeddings_array = np.array([chunk[\"embedding\"] for chunk in chunks_with_embeddings])\n",
        "            dimension = embeddings_array.shape[1]\n",
        "            faiss_index = faiss.IndexFlatL2(dimension)\n",
        "            faiss_index.add(embeddings_array.astype('float32'))\n",
        "            \n",
        "            # Create BM25 index\n",
        "            tokenized_texts = [text.split() for text in texts]\n",
        "            bm25_index = BM25Okapi(tokenized_texts)\n",
        "            \n",
        "            # Load reranker\n",
        "            reranker = load_reranker()\n",
        "            \n",
        "            # Create hybrid retriever\n",
        "            retriever = HybridRetriever(embedding_model, faiss_index, bm25_index, {\"chunks\": chunks_with_embeddings}, reranker)\n",
        "            \n",
        "            # Load Mistral model\n",
        "            mistral_model, mistral_tokenizer = load_mistral_model()\n",
        "            \n",
        "            # Create enhanced RAG system\n",
        "            st.session_state.rag_system = EnhancedRAGSystem(retriever, mistral_model, mistral_tokenizer)\n",
        "    \n",
        "    # Display file information\n",
        "    st.success(f\"PDFs processed successfully!\")\n",
        "    st.info(f\"Total chunks: {len(chunks) if 'chunks' in locals() else 'N/A'}\")\n",
        "    \n",
        "    # Chat interface\n",
        "    st.markdown('<div class=\"chat-area\">', unsafe_allow_html=True)\n",
        "    \n",
        "    # Display messages\n",
        "    for turn in st.session_state.conversation:\n",
        "        if turn[\"role\"] == \"user\":\n",
        "            st.markdown(f'''<div class=\"message user\"><div class=\"message-content\">{turn[\"text\"]}</div></div>''', unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.markdown(f'''<div class=\"message bot\"><div class=\"message-content\">{turn[\"text\"]}</div></div>''', unsafe_allow_html=True)\n",
        "    \n",
        "    # Show typing indicator\n",
        "    if st.session_state.processing:\n",
        "        st.markdown('<div class=\"typing\">Bot is thinking...</div>', unsafe_allow_html=True)\n",
        "    \n",
        "    st.markdown('</div>', unsafe_allow_html=True)\n",
        "    \n",
        "    # Input area\n",
        "    st.markdown('<div class=\"input-area\">', unsafe_allow_html=True)\n",
        "    \n",
        "    # Use dynamic key to clear input field\n",
        "    if \"input_key\" not in st.session_state:\n",
        "        st.session_state.input_key = 0\n",
        "    \n",
        "    if st.session_state.get(\"input_cleared\", False):\n",
        "        st.session_state.input_key += 1\n",
        "        st.session_state.input_cleared = False\n",
        "    \n",
        "    user_input = st.text_input(\"\", placeholder=\"Ask a question about your PDFs...\", key=f\"user_input_{st.session_state.input_key}\", disabled=st.session_state.processing)\n",
        "    st.markdown('</div>', unsafe_allow_html=True)\n",
        "\n",
        "    # Handle input submission\n",
        "    if user_input and not st.session_state.processing:\n",
        "        if \"last_input\" not in st.session_state or st.session_state.last_input != user_input:\n",
        "            st.session_state.conversation.append({\"role\": \"user\", \"text\": user_input})\n",
        "            st.session_state.processing = True\n",
        "            st.session_state.last_input = user_input\n",
        "            st.session_state.input_cleared = True\n",
        "            st.rerun()\n",
        "    \n",
        "    # Process bot response\n",
        "    if st.session_state.processing and len(st.session_state.conversation) > 0 and st.session_state.conversation[-1][\"role\"] == \"user\":\n",
        "        with st.spinner(\"Generating response...\"):\n",
        "            user_question = st.session_state.conversation[-1]['text']\n",
        "            result = st.session_state.rag_system.answer_question(user_question, k=5)\n",
        "            \n",
        "            # Store result for visualization\n",
        "            st.session_state.last_result = result\n",
        "            \n",
        "            # Only store the clean answer\n",
        "            st.session_state.conversation.append({\"role\": \"bot\", \"text\": result[\"answer\"]})\n",
        "            st.session_state.processing = False\n",
        "            st.rerun()\n",
        "    \n",
        "    # Show retrieval visualization if available\n",
        "    if \"last_result\" in st.session_state and st.session_state.last_result:\n",
        "        with st.expander(\"üîç Retrieval Analysis\", expanded=False):\n",
        "            st.write(\"**Retrieved Chunks with Scores:**\")\n",
        "            \n",
        "            for i, chunk in enumerate(st.session_state.last_result[\"retrieved_chunks\"]):\n",
        "                col1, col2 = st.columns([3, 1])\n",
        "                \n",
        "                with col1:\n",
        "                    st.write(f\"**Chunk {i+1}:** {chunk['text'][:200]}...\")\n",
        "                \n",
        "                with col2:\n",
        "                    scores = st.session_state.last_result[\"retrieval_scores\"][f\"chunk_{i+1}\"]\n",
        "                    st.metric(\"Dense Score\", f\"{scores['dense_score']:.3f}\")\n",
        "                    st.metric(\"Sparse Score\", f\"{scores['sparse_score']:.3f}\")\n",
        "                    st.metric(\"Final Score\", f\"{scores['final_score']:.3f}\")\n",
        "                \n",
        "                st.divider()\n",
        "            \n",
        "            # Show score distribution\n",
        "            st.write(\"**Score Distribution:**\")\n",
        "            import pandas as pd\n",
        "            import plotly.express as px\n",
        "            \n",
        "            score_data = []\n",
        "            for i, chunk in enumerate(st.session_state.last_result[\"retrieved_chunks\"]):\n",
        "                scores = st.session_state.last_result[\"retrieval_scores\"][f\"chunk_{i+1}\"]\n",
        "                score_data.append({\n",
        "                    \"Chunk\": f\"Chunk {i+1}\",\n",
        "                    \"Dense\": scores['dense_score'],\n",
        "                    \"Sparse\": scores['sparse_score'],\n",
        "                    \"Final\": scores['final_score']\n",
        "                })\n",
        "            \n",
        "            df = pd.DataFrame(score_data)\n",
        "            fig = px.bar(df, x=\"Chunk\", y=[\"Dense\", \"Sparse\", \"Final\"], \n",
        "                        title=\"Retrieval Scores by Chunk\", barmode='group')\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "st.markdown('</div>', unsafe_allow_html=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the Application\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Kill any existing processes\n",
        "os.system(\"pkill -f streamlit\")\n",
        "ngrok.kill()\n",
        "time.sleep(2)\n",
        "\n",
        "# Start Streamlit\n",
        "subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.headless=true\"])\n",
        "time.sleep(5)\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"RAG PDF Chat System URL:\", public_url)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
